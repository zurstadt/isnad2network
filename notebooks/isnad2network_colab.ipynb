# -*- coding: utf-8 -*-
"""isnad2network_colab.py - Improved Version with Persistent File Management

This script integrates three main components for processing isnad (chain of transmission) data:
1. Name Replacement: Processes network pathways and replaces node names based on mappings
2. Dictionary Creation: Creates dictionary files for machine learning training
3. Network JSON Generation: Generates structured JSON files for network analysis

This version maintains uploaded files across iterations and avoids infinite loops.
"""

# Import required libraries
import os
import sys
import time
import json
import glob
import shutil
import zipfile
import traceback
import importlib.util
import logging
from datetime import datetime

# Set up basic logging
log_dir = "logs"
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('isnad_pipeline')

# -----------------------------------------------------------------
# Utility Functions
# -----------------------------------------------------------------

def is_colab():
    """Check if running in Google Colab environment"""
    try:
        import google.colab
        return True
    except ImportError:
        return False


def clean_colab_environment():
    """Clean up unnecessary files in Colab, but preserve data files"""
    if not is_colab():
        print("Not running in Colab. Skipping cleanup.")
        return

    print("Cleaning Colab environment...")
    
    # Save file list before cleanup - we'll preserve data files
    preserve_extensions = ['.csv', '.py', '.ipynb', '.json']
    preserved_files = {}
    
    for ext in preserve_extensions:
        for file in glob.glob(f'*{ext}'):
            # Read file content
            try:
                with open(file, 'rb') as f:
                    preserved_files[file] = f.read()
                print(f"  Preserving {file}")
            except:
                print(f"  Failed to preserve {file}")
    
    # Create required directories
    os.makedirs('output', exist_ok=True)
    os.makedirs('output/network', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    # Restore preserved files
    for file, content in preserved_files.items():
        try:
            with open(file, 'wb') as f:
                f.write(content)
            print(f"  Restored {file}")
        except:
            print(f"  Failed to restore {file}")
    
    print("Environment cleaned.")


def setup_output_directories():
    """Create necessary output directories"""
    # Define directory structure
    base_dir = f"isnad_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    dirs = {
        "data": os.path.join(base_dir, "data"),
        "networks": os.path.join(base_dir, "networks"),
        "dictionaries": os.path.join(base_dir, "dictionaries"),
        "logs": os.path.join(base_dir, "logs"),
    }
    
    # Create directories
    for dir_path in dirs.values():
        os.makedirs(dir_path, exist_ok=True)
        
    return base_dir, dirs


def load_module(name, path):
    """Load a module from a file path"""
    try:
        print(f"Loading {name} from {path}...")
        spec = importlib.util.spec_from_file_location(name, path)
        if spec is None:
            print(f"Could not create spec for module {name}")
            return None

        module = importlib.util.module_from_spec(spec)
        sys.modules[name] = module
        spec.loader.exec_module(module)
        print(f"Successfully loaded {name}")
        return module
    except Exception as e:
        print(f"Failed to import {name} module: {e}")
        traceback.print_exc()
        return None


def create_zip_archive(base_dir):
    """Create a ZIP archive of the output directory"""
    try:
        zip_filename = f"{base_dir}.zip"
        print(f"Creating ZIP archive: {zip_filename}")
        
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(base_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, os.path.dirname(base_dir))
                    zipf.write(file_path, arcname)
        
        print(f"ZIP archive created successfully: {zip_filename}")
        
        # Download if in Colab
        if is_colab():
            try:
                from google.colab import files
                files.download(zip_filename)
                print(f"Download initiated for {zip_filename}")
            except Exception as e:
                print(f"Error downloading ZIP file: {e}")
                print("To download manually, run this code in a new cell:")
                print(f"from google.colab import files; files.download('{zip_filename}')")
        
        return zip_filename
    except Exception as e:
        print(f"Error creating ZIP file: {e}")
        traceback.print_exc()
        return None


def upload_files(required_files, optional_files=None):
    """
    Upload files with persistent storage, returning a dictionary of successful uploads.
    
    Args:
        required_files: List of required file names
        optional_files: List of optional file names
        
    Returns:
        Dictionary mapping file names to paths
    """
    if optional_files is None:
        optional_files = []
    
    # Initialize with existing files
    file_paths = {}
    for file in required_files + optional_files:
        if os.path.exists(file):
            file_paths[file] = file
    
    # Display current status
    print("\nCurrent file status:")
    print("Required files:")
    for file in required_files:
        status = "✓ Found" if file in file_paths else "✗ Missing"
        print(f"  - {file}: {status}")
    
    if optional_files:
        print("Optional files:")
        for file in optional_files:
            status = "✓ Found" if file in file_paths else "✗ Missing"
            print(f"  - {file}: {status}")
    
    # Check if all required files are available
    missing_required = [f for f in required_files if f not in file_paths]
    
    if not missing_required:
        print("\n✓ All required files are already available.")
        proceed = input("Would you like to upload additional files anyway? (y/n): ")
        if proceed.lower() != 'y':
            return file_paths
    
    # If in Colab, provide upload functionality
    if is_colab():
        from google.colab import files
        
        print("\nClick 'Choose Files' to upload missing files:")
        try:
            uploaded = files.upload()
            
            if not uploaded:
                print("No files were uploaded.")
            else:
                # Process uploaded files
                for uploaded_name, content in uploaded.items():
                    # Check if the file matches a required or optional name
                    all_files = required_files + optional_files
                    matched = False
                    
                    for target_name in all_files:
                        # Extract base name without path or suffixes
                        uploaded_base = os.path.splitext(uploaded_name)[0].split(' (')[0]
                        target_base = os.path.splitext(target_name)[0]
                        
                        if uploaded_base == target_base:
                            # Save with the target name
                            with open(target_name, 'wb') as f:
                                f.write(content)
                            
                            file_paths[target_name] = target_name
                            print(f"✓ Matched {uploaded_name} to {target_name} and saved")
                            matched = True
                            break
                    
                    # If no match, save with original name
                    if not matched:
                        with open(uploaded_name, 'wb') as f:
                            f.write(content)
                        print(f"✓ Saved unmatched file: {uploaded_name}")
        except Exception as e:
            print(f"Error during upload: {e}")
    
    # Update status after uploads
    print("\nUpdated file status:")
    print("Required files:")
    for file in required_files:
        status = "✓ Found" if os.path.exists(file) else "✗ Missing"
        if os.path.exists(file):
            file_paths[file] = file
        print(f"  - {file}: {status}")
    
    if optional_files:
        print("Optional files:")
        for file in optional_files:
            status = "✓ Found" if os.path.exists(file) else "✗ Missing"
            if os.path.exists(file):
                file_paths[file] = file
            print(f"  - {file}: {status}")
    
    # Check if we can proceed
    missing_required = [f for f in required_files if f not in file_paths]
    if missing_required:
        print("\n⚠️ Some required files are still missing:")
        for file in missing_required:
            print(f"  - {file}")
        
        # Ask if user wants to upload more
        upload_more = input("\nWould you like to upload more files? (y/n): ")
        if upload_more.lower() == 'y':
            return upload_files(required_files, optional_files)
    
    return file_paths


# -----------------------------------------------------------------
# Pipeline Functions
# -----------------------------------------------------------------

def run_name_replacement(name_replacer_module, names_file, nodelist_file, output_file):
    """Run the name replacement step"""
    print("\nRunning name replacement step...")
    
    try:
        result_df = name_replacer_module.process_network_names(
            names_file=names_file,
            nodelist_file=nodelist_file,
            output_file=output_file
        )
        
        if result_df is not None and not result_df.empty:
            print(f"✓ Name replacement completed successfully: {len(result_df)} rows processed")
            print(f"✓ Output saved to: {output_file}")
            return True, result_df
        else:
            print("✗ Name replacement failed or returned empty results")
            return False, None
    except Exception as e:
        print(f"✗ Error in name replacement: {str(e)}")
        traceback.print_exc()
        return False, None


def run_dictionary_creation(dict_creator_module, names_replaced_file, dict_dir):
    """Run the dictionary creation step"""
    print("\nRunning dictionary creation step...")
    
    try:
        # Create processor
        processor = dict_creator_module.CSVDictionaryProcessor()
        processor.input_file = names_replaced_file
        
        # Set output path
        base_filename = os.path.splitext(os.path.basename(names_replaced_file))[0]
        processor.filename_base = os.path.join(dict_dir, base_filename)
        
        # Load CSV
        if processor.load_csv():
            print(f"✓ Loaded data: {processor.data.shape[0]} rows × {processor.data.shape[1]} columns")
            
            # Find t-columns
            t_columns = [col for col in processor.data.columns 
                        if col.startswith('t') and col not in ['path_id', 'isnad_id']]
            
            if not t_columns:
                print("✗ No t-columns found in the CSV")
                return False, {}
            
            print(f"✓ Found {len(t_columns)} t-columns")
            
            # Create dictionaries
            unique_file = processor.create_unique_values_dict(t_columns)
            annotate_file = processor.create_annotation_dict(t_columns)
            
            output_files = {}
            if unique_file:
                output_files['dict_unique'] = unique_file
                print(f"✓ Created unique values dictionary: {unique_file}")
            
            if annotate_file:
                output_files['dict_annotate'] = annotate_file
                print(f"✓ Created annotation dictionary: {annotate_file}")
            
            if output_files:
                return True, output_files
            else:
                print("✗ Failed to create dictionary files")
                return False, {}
        else:
            print(f"✗ Failed to load {names_replaced_file}")
            return False, {}
    except Exception as e:
        print(f"✗ Error in dictionary creation: {str(e)}")
        traceback.print_exc()
        return False, {}


def run_network_generation(network_generator_module, transmission_file, names_replaced_file, 
                          metadata_file, network_dir):
    """Run the network JSON generation step"""
    print("\nRunning network generation step...")
    
    try:
        # Check for process_isnad_network function
        if hasattr(network_generator_module, 'process_isnad_network'):
            process_result = network_generator_module.process_isnad_network(
                trans_file=transmission_file,
                names_file=names_replaced_file,
                metadata_file=metadata_file,
                output_dir=network_dir,
                skip_filtering=False
            )
            
            if process_result and process_result.get("status") == "success":
                output_files = process_result.get('output_files', {})
                print(f"✓ Network generation completed successfully")
                for name, path in output_files.items():
                    print(f"  - {name}: {path}")
                return True, output_files
            else:
                print(f"✗ Network generation failed")
                return False, {}
        else:
            print("✗ process_isnad_network function not found in module")
            return False, {}
    except Exception as e:
        print(f"✗ Error in network generation: {str(e)}")
        traceback.print_exc()
        return False, {}


# -----------------------------------------------------------------
# Main Process
# -----------------------------------------------------------------

def run_pipeline():
    """
    Run the isnad network processing pipeline with persistent file management.
    This approach avoids infinite loops while maintaining uploaded files.
    """
    print("\n" + "=" * 60)
    print("ISNAD NETWORK PROCESSING PIPELINE")
    print("=" * 60)
    
    # Clean environment in Colab (preserving data files)
    if is_colab():
        clean_colab_environment()
    
    # Define required and optional files
    required_scripts = [
        'match_replace_isnads.py', 
        'generate_json_network_isnad.py'
    ]
    
    optional_scripts = [
        '2dict.py'
    ]
    
    required_data = [
        'names.csv', 
        'nodelist.csv', 
        'transmissionterms.csv'
    ]
    
    optional_data = [
        'pathmetadata.csv'
    ]
    
    # Upload scripts first
    print("\nSTEP 1: UPLOAD SCRIPT FILES")
    print("=" * 30)
    print("The following script files are needed:")
    print("Required:")
    for script in required_scripts:
        print(f"  - {script}")
    print("Optional:")
    for script in optional_scripts:
        print(f"  - {script}")
    
    script_files = upload_files(required_scripts, optional_scripts)
    
    # If missing required scripts, exit
    missing_scripts = [s for s in required_scripts if s not in script_files]
    if missing_scripts:
        print("\n✗ Missing required script files:")
        for script in missing_scripts:
            print(f"  - {script}")
        print("Pipeline cannot proceed without these files.")
        return
    
    # Upload data files
    print("\nSTEP 2: UPLOAD DATA FILES")
    print("=" * 30)
    print("The following data files are needed:")
    print("Required:")
    for data_file in required_data:
        print(f"  - {data_file}")
    print("Optional:")
    for data_file in optional_data:
        print(f"  - {data_file}")
    
    data_files = upload_files(required_data, optional_data)
    
    # If missing required data, exit
    missing_data = [d for d in required_data if d not in data_files]
    if missing_data:
        print("\n✗ Missing required data files:")
        for data_file in missing_data:
            print(f"  - {data_file}")
        print("Pipeline cannot proceed without these files.")
        return
    
    # Load modules
    modules = {}
    for script_path in script_files.values():
        if script_path == 'match_replace_isnads.py':
            modules['name_replacer'] = load_module('name_replacer', script_path)
        elif script_path == '2dict.py':
            modules['dict_creator'] = load_module('dict_creator', script_path)
        elif script_path == 'generate_json_network_isnad.py':
            modules['network_generator'] = load_module('network_generator', script_path)
    
    # Check if required modules loaded successfully
    if 'name_replacer' not in modules or 'network_generator' not in modules:
        print("\n✗ Failed to load required modules")
        return
    
    # Setup output directories
    base_dir, dirs = setup_output_directories()
    print(f"\nOutput will be stored in: {os.path.abspath(base_dir)}")
    
    # Ask for confirmation before proceeding
    proceed = input("\nAll files are ready. Proceed with processing? (y/n): ")
    if proceed.lower() != 'y':
        print("Pipeline execution cancelled.")
        return
    
    # Run name replacement
    names_replaced_file = os.path.join(dirs['data'], "names_replaced.csv")
    success, _ = run_name_replacement(
        modules['name_replacer'],
        data_files['names.csv'],
        data_files['nodelist.csv'],
        names_replaced_file
    )
    
    if not success:
        print("\n✗ Name replacement failed. Cannot continue.")
        return
    
    # Run dictionary creation if available
    if 'dict_creator' in modules:
        run_dictionary_creation(
            modules['dict_creator'],
            names_replaced_file,
            dirs['dictionaries']
        )
        # Continue even if dictionary creation fails
    
    # Run network generation
    metadata_file = data_files.get('pathmetadata.csv')
    success, _ = run_network_generation(
        modules['network_generator'],
        data_files['transmissionterms.csv'],
        names_replaced_file,
        metadata_file,
        dirs['networks']
    )
    
    if not success:
        print("\n✗ Network generation failed.")
        return
    
    # Create ZIP archive
    zip_file = create_zip_archive(base_dir)
    
    if zip_file:
        print("\n" + "=" * 60)
        print("PIPELINE COMPLETED SUCCESSFULLY")
        print("=" * 60)
        print(f"Results saved to: {os.path.abspath(zip_file)}")
    else:
        print("\n" + "=" * 60)
        print("PIPELINE COULD NOT CREATE ZIP FILE")
        print("=" * 60)
        print(f"Results are in directory: {os.path.abspath(base_dir)}")


if __name__ == "__main__":
    try:
        run_pipeline()
    except KeyboardInterrupt:
        print("\n\nPipeline interrupted by user. Exiting.")
    except Exception as e:
        print(f"\n\nAn error occurred: {str(e)}")
        traceback.print_exc()