# -*- coding: utf-8 -*-
"""2dict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sCt2jM8AuvDJHX1D6ckO6eniEGu5W11O
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
CSV Dictionary Processing Script
--------------------------------
This script processes CSV files to create two output files:
1. A CSV with unique values from specified columns
2. A CSV with unique strings (split by whitespace), their counts, and a 'training' column

Optimized for Google Colab environment.
"""

import pandas as pd
import numpy as np
import os
import re
import time
import logging
from tqdm.notebook import tqdm
try:
    # Import files dynamically to avoid errors when not running in Colab
    from google.colab import files
except ImportError:
    pass  # Not running in Colab
import io

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('csv_processing.log')
    ]
)
logger = logging.getLogger('csv_processor')

class CSVDictionaryProcessor:
    """Class to process CSV files and create dictionary outputs."""

    def __init__(self):
        """Initialize the processor."""
        self.input_file = None
        self.data = None
        self.filename_base = None

    def upload_file(self):
        """Upload a CSV file in Google Colab."""
        logger.info("Waiting for file upload...")
        uploaded = {} # File upload handled by main notebook

        if not uploaded:
            logger.error("No file was uploaded.")
            return False

        self.input_file = list(uploaded.keys())[0]
        self.filename_base = os.path.splitext(self.input_file)[0]
        logger.info(f"File '{self.input_file}' uploaded successfully.")
        return True

    def load_csv(self, encoding='utf-8', sep=','):
        """
        Load the CSV file and display a preview.

        Args:
            encoding (str): Character encoding of the file
            sep (str): Delimiter in the CSV file

        Returns:
            bool: True if loaded successfully, False otherwise
        """
        if not self.input_file:
            logger.error("No input file specified. Please upload a file first.")
            return False

        try:
            logger.info(f"Loading file: {self.input_file}")
            start_time = time.time()

            # Progress reporting using tqdm for file reading
            with open(self.input_file, 'rb') as f:
                content = f.read()

            # Attempt to load with specified encoding
            try:
                self.data = pd.read_csv(io.BytesIO(content), encoding=encoding, sep=sep)
            except UnicodeDecodeError:
                # Try with utf-8-sig if utf-8 fails
                logger.warning(f"Failed to decode with {encoding}, trying with utf-8-sig")
                self.data = pd.read_csv(io.BytesIO(content), encoding='utf-8-sig', sep=sep)

            load_time = time.time() - start_time
            rows, cols = self.data.shape

            logger.info(f"File loaded successfully in {load_time:.2f} seconds.")
            logger.info(f"Dataset dimensions: {rows} rows Ã— {cols} columns")

            # Display preview and column information
            print("\n--- Data Preview ---")
            display(self.data.head())

            print("\n--- Column Information ---")
            for i, col in enumerate(self.data.columns):
                print(f"{i+1}. {col} - {self.data[col].dtype}")

            return True

        except Exception as e:
            logger.error(f"Error loading file: {str(e)}")
            return False

    def create_unique_values_dict(self, columns):
        """
        Create a CSV with unique values from specified columns in a single column.

        Args:
            columns (list): List of column names to extract unique values from

        Returns:
            str: Path to the created file
        """
        if self.data is None:
            logger.error("No data loaded. Please load a CSV file first.")
            return None

        # Validate columns
        invalid_cols = [col for col in columns if col not in self.data.columns]
        if invalid_cols:
            logger.error(f"Invalid column(s): {', '.join(invalid_cols)}")
            return None

        output_file = f"{self.filename_base}_dict_unique.csv"
        logger.info(f"Creating unique values dictionary from columns: {columns}")

        try:
            # Collect all values from all specified columns
            all_values = []

            for col in tqdm(columns, desc="Processing columns"):
                unique_vals = self.data[col].dropna().unique()
                all_values.extend(unique_vals)

            # Remove duplicates
            unique_values = list(set(all_values))

            # Create DataFrame with a single column for unique values
            result_df = pd.DataFrame({
                'unique_names': unique_values
            })

            # Sort values alphabetically
            result_df = result_df.sort_values('unique_names').reset_index(drop=True)

            # Save to CSV
            result_df.to_csv(output_file, index=False, encoding='utf-8')
            logger.info(f"Unique values dictionary saved to '{output_file}'")

            # Download file in Google Colab
            files.download(output_file)

            return output_file

        except Exception as e:
            logger.error(f"Error creating unique values dictionary: {str(e)}")
            return None


    def create_annotation_dict(self, columns):
        """
        Create a CSV with unique strings (split by whitespace), their counts, and annotation columns.

        Args:
            columns (list): List of column names to extract strings from

        Returns:
            str: Path to the created file
        """
        if self.data is None:
            logger.error("No data loaded. Please load a CSV file first.")
            return None

        # Validate columns
        invalid_cols = [col for col in columns if col not in self.data.columns]
        if invalid_cols:
            logger.error(f"Invalid column(s): {', '.join(invalid_cols)}")
            return None

        output_file = f"{self.filename_base}_dict_annotate.csv"
        logger.info(f"Creating annotation dictionary from columns: {columns}")

        try:
            # Extract all words from all specified columns
            all_words = []

            for column in columns:
                logger.info(f"Processing column: {column}")
                # Extract the column data
                column_data = self.data[column].dropna().astype(str)

                # Split by whitespace and create a flat list of all words
                for text in tqdm(column_data, desc=f"Extracting words from {column}"):
                    # Split by whitespace
                    words = re.findall(r'\S+', text)
                    all_words.extend(words)

            # Count occurrences of each word
            word_counts = {}
            for word in tqdm(all_words, desc="Counting words"):
                word_counts[word] = word_counts.get(word, 0) + 1

            # Create DataFrame with words, counts, and empty annotation columns
            result_df = pd.DataFrame({
                'unique_strings': list(word_counts.keys()),
                'count': list(word_counts.values()),
                'annotate_ar': '',
                'annotate_kunyah': '',
                'annotate_nasab': '',
                'annotate_nisbah': ''
            })

            # Sort by count (descending)
            result_df = result_df.sort_values('count', ascending=False).reset_index(drop=True)

            # Save to CSV
            result_df.to_csv(output_file, index=False, encoding='utf-8')
            logger.info(f"Annotation dictionary saved to '{output_file}'")

            # Download file in Google Colab
            files.download(output_file)

            return output_file

        except Exception as e:
            logger.error(f"Error creating annotation dictionary: {str(e)}")
            return None


def main():
    """Main function to run the CSV processor."""
    print("=" * 50)
    print("CSV Dictionary Processor")
    print("=" * 50)

    processor = CSVDictionaryProcessor()

    # Step 1: Upload and load file
    if not processor.upload_file():
        return

    if not processor.load_csv():
        return

    # Step 2: Get column selection (used for both outputs)
    print("\n" + "=" * 50)
    print("Column Selection")
    print("=" * 50)

    # Get t-prefixed columns as default option
    t_columns = [col for col in processor.data.columns if col.startswith('t')]

    if t_columns:
        t_columns_str = ', '.join(t_columns)
        print(f"\nDefault option: Columns starting with 't': {t_columns_str}")
        use_default = input("Use these columns? (y/n): ").strip().lower()

        if use_default == 'y':
            columns = t_columns
        else:
            # Get custom column selection from user
            while True:
                column_input = input("\nEnter column names to extract unique values from (comma-separated): ")
                columns = [col.strip() for col in column_input.split(',')]

                # Validate columns
                invalid_cols = [col for col in columns if col not in processor.data.columns]
                if invalid_cols:
                    print(f"Invalid column(s): {', '.join(invalid_cols)}")
                    print("Available columns:")
                    for i, col in enumerate(processor.data.columns):
                        print(f"{i+1}. {col}")
                else:
                    break
    else:
        print("\nNo columns starting with 't' found.")
        # Get custom column selection from user
        while True:
            column_input = input("\nEnter column names to extract unique values from (comma-separated): ")
            columns = [col.strip() for col in column_input.split(',')]

            # Validate columns
            invalid_cols = [col for col in columns if col not in processor.data.columns]
            if invalid_cols:
                print(f"Invalid column(s): {', '.join(invalid_cols)}")
                print("Available columns:")
                for i, col in enumerate(processor.data.columns):
                    print(f"{i+1}. {col}")
            else:
                break

    # Step 3: Process for unique values dictionary
    print("\n" + "=" * 50)
    print("Creating Unique Values Dictionary")
    print("=" * 50)

    # Create unique values dictionary from all selected columns
    unique_file = processor.create_unique_values_dict(columns)

    # Step 4: Process for annotation dictionary
    print("\n" + "=" * 50)
    print("Creating Annotation Dictionary")
    print("=" * 50)

    print(f"Using the same columns selected earlier: {', '.join(columns)}")

    # Create annotation dictionary for all selected columns
    annotate_file = processor.create_annotation_dict(columns)

    # Summary
    if unique_file and annotate_file:
        print("\n" + "=" * 50)
        print("Processing Complete")
        print("=" * 50)
        print(f"1. Unique values dictionary: {unique_file}")
        print(f"2. Annotation dictionary: {annotate_file}")
        print("\nFiles have been downloaded. Check your downloads folder.")

if __name__ == "__main__":
    main()